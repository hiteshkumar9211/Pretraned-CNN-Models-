{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2bc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9617dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader for Tonji\n",
    "class PalmROIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_list = sorted([f for f in os.listdir(root_dir) if f.endswith(\".bmp\")])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.resize(image, (128, 128))  \n",
    "        image = image[..., np.newaxis]  \n",
    "        image = image.astype(np.float32) / 255.0  \n",
    "        image = torch.from_numpy(image).permute(2, 0, 1) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Extract person ID (e.g., '00001.bmp' to '00020.bmp' -> person 1)\n",
    "        try:\n",
    "            img_number = int(img_name.split(\".\")[0])  \n",
    "            person_id = ((img_number - 1) // 20) + 1 \n",
    "            label = person_id - 1  \n",
    "            if not (0 <= label <= 299):\n",
    "                raise ValueError(f\"Label {label + 1} out of range (1 to 300) for {img_name}\")\n",
    "        except (ValueError, IndexError):\n",
    "            raise ValueError(f\"Invalid filename format for label: {img_name}\")\n",
    "        return image, label, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e5ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 with Batch Normalization (pretrained, modified for grayscale and 128x128 input)\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=300):\n",
    "        super(VGG16, self).__init__()\n",
    "        # Load pretrained VGG-16 with batch normalization\n",
    "        vgg16_bn = models.vgg16_bn(pretrained=True)\n",
    "        # Modify first convolutional layer for 1-channel input\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  \n",
    "            *vgg16_bn.features[1:]  \n",
    "        )\n",
    "        # Copy pretrained weights for conv1 (average RGB channels)\n",
    "        with torch.no_grad():\n",
    "            self.features[0].weight.copy_(vgg16_bn.features[0].weight.mean(dim=1, keepdim=True))\n",
    "            self.features[0].bias.copy_(vgg16_bn.features[0].bias)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))  \n",
    "        self.classifier = vgg16_bn.classifier\n",
    "        # Modify classifier for 300 classes (input size adjusted for 4x4 pooling)\n",
    "        self.classifier[0] = nn.Linear(512 * 4 * 4, 4096)  \n",
    "        self.classifier[-1] = nn.Linear(4096, num_classes)\n",
    "        # Initialize new classifier layers\n",
    "        nn.init.xavier_uniform_(self.classifier[0].weight)\n",
    "        nn.init.zeros_(self.classifier[0].bias)\n",
    "        nn.init.xavier_uniform_(self.classifier[-1].weight)\n",
    "        nn.init.zeros_(self.classifier[-1].bias)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if return_features:\n",
    "            x = self.classifier[:6](x)  \n",
    "            return x\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85583d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from a dataloader\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    img_names = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, names in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            x = x.to(device)\n",
    "            f = model(x, return_features=True).cpu().numpy()\n",
    "            features_list.append(f)\n",
    "            labels_list.extend(y.numpy())\n",
    "            img_names.extend(names)\n",
    "    features = np.concatenate(features_list, axis=0)\n",
    "    labels = np.array(labels_list)\n",
    "    return features, labels, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3663c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune VGG-16 on Tonji\n",
    "def fine_tune_vgg16(model, train_loader, device, epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Differential learning rates for pretrained and new layers\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.features[0].parameters(), 'lr': 0.001},  \n",
    "        {'params': model.features[1:].parameters(), 'lr': 0.0001},  \n",
    "        {'params': model.classifier.parameters(), 'lr': 0.001},  \n",
    "    ], lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    model.train()\n",
    "    prev_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y, _ in tqdm(train_loader, desc=f\"Fine-Tuning Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Fine-Tuning Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        scheduler.step(avg_loss)\n",
    "        # Log learning rate changes\n",
    "        current_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "        if any(lr != prev_lr[i] for i, lr in enumerate(current_lr)):\n",
    "            print(f\"Epoch {epoch+1}: Learning rates updated to {current_lr}\")\n",
    "        prev_lr = current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f867ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics (Top-1, Top-5, ROC AUC, EER, FAR, FRR)\n",
    "def compute_metrics(features, labels, clf):\n",
    "    decision_scores = clf.decision_function(features)  \n",
    "    topk_acc = {}\n",
    "    for k in [1, 5]:\n",
    "        top_k_indices = np.argsort(decision_scores, axis=1)[:, -k:]\n",
    "        correct = 0\n",
    "        for i, top_k in enumerate(top_k_indices):\n",
    "            if labels[i] in top_k:\n",
    "                correct += 1\n",
    "        topk_acc[k] = correct / len(labels)\n",
    "    y_true, y_score = [], []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            score_i = decision_scores[i, labels[i]]\n",
    "            score_j = decision_scores[j, labels[j]]\n",
    "            sim = score_i + score_j\n",
    "            y_score.append(sim)\n",
    "            y_true.append(1 if labels[i] == labels[j] else 0)\n",
    "    if len(set(y_true)) <= 1:\n",
    "        print(\"Warning: y_true contains only one class. Setting ROC AUC, EER, FAR, FRR to 0.\")\n",
    "        roc_auc = 0.1\n",
    "        eer = 0.1\n",
    "        far = 0.1\n",
    "        frr = 0.1\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        diff = np.abs(fnr - fpr)\n",
    "        if np.all(np.isnan(diff)):\n",
    "            print(\"Warning: All-NaN slice in EER calculation. Setting EER, FAR, FRR to 0.\")\n",
    "            eer = 0.1\n",
    "            far = 0.1\n",
    "            frr = 0.1\n",
    "        else:\n",
    "            eer_idx = np.nanargmin(diff)\n",
    "            eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "            far = fpr[eer_idx]\n",
    "            frr = fnr[eer_idx]\n",
    "    return topk_acc, roc_auc, eer, far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63dfd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code\n",
    "session1_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session1\"\n",
    "session2_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93424997",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e32775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for Tongji\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b40b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tongji datasets\n",
    "dataset1 = PalmROIDataset(session1_root, transform=transform)\n",
    "dataset2 = PalmROIDataset(session2_root, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab52773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 images: 6000\n",
      "Session 2 images: 6000\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset sizes\n",
    "print(f\"Session 1 images: {len(dataset1)}\")\n",
    "print(f\"Session 2 images: {len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37c719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 12000\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "full_dataset = ConcatDataset([dataset1, dataset2])\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23c7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a7ab087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe42d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiteshk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiteshk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Fine-Tuning Epoch 1: 100%|██████████| 300/300 [01:59<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 1, Loss: 5.7191, Accuracy: 0.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2: 100%|██████████| 300/300 [01:46<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2, Loss: 5.0985, Accuracy: 2.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3: 100%|██████████| 300/300 [01:49<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3, Loss: 3.6099, Accuracy: 17.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4: 100%|██████████| 300/300 [01:49<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4, Loss: 1.7448, Accuracy: 54.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5: 100%|██████████| 300/300 [01:47<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5, Loss: 0.6476, Accuracy: 81.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6: 100%|██████████| 300/300 [01:48<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6, Loss: 0.2919, Accuracy: 91.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7: 100%|██████████| 300/300 [01:48<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7, Loss: 0.1949, Accuracy: 94.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8: 100%|██████████| 300/300 [01:51<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8, Loss: 0.1835, Accuracy: 94.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9: 100%|██████████| 300/300 [01:49<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9, Loss: 0.1186, Accuracy: 96.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10: 100%|██████████| 300/300 [01:48<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10, Loss: 0.1324, Accuracy: 96.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fine-tune VGG-16\n",
    "model = VGG16(num_classes=300).to(device)\n",
    "fine_tune_vgg16(model, train_loader, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75349f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 300/300 [00:51<00:00,  5.80it/s]\n",
      "Extracting features: 100%|██████████| 75/75 [00:15<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "train_features, train_labels, _ = extract_features(model, train_loader, device)\n",
    "test_features, test_labels, _ = extract_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9366f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 300\n"
     ]
    }
   ],
   "source": [
    "# Verify number of classes\n",
    "num_classes = len(np.unique(np.concatenate((train_labels, test_labels))))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "if num_classes != 300:\n",
    "    raise ValueError(f\"Expected 300 classes, but found {num_classes}. Verify filename format (e.g., '00001.bmp' to '12000.bmp').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60acb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "test_features = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec77646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF kernel and hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a1d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 10, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr', probability=False)\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f\"Best SVM parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "580ed26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator\n",
    "svm_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de75e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "topk_acc, roc_auc, eer, far, frr = compute_metrics(test_features, test_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38998c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Identification Accuracy: 99.54%\n",
      "Top-5 Identification Accuracy: 99.79%\n",
      "ROC AUC: 0.4984\n",
      "EER: 0.5030\n",
      "FAR: 0.5029\n",
      "FRR: 0.5030\n"
     ]
    }
   ],
   "source": [
    "#Print metrics\n",
    "print(f\"Top-1 Identification Accuracy: {topk_acc[1]*100:.2f}%\")\n",
    "print(f\"Top-5 Identification Accuracy: {topk_acc[5]*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "print(f\"FAR: {far:.4f}\")\n",
    "print(f\"FRR: {frr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
