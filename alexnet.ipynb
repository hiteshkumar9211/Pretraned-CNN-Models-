{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990b0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9dc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader for Tongji\n",
    "class PalmROIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_list = sorted([f for f in os.listdir(root_dir) if f.endswith(\".bmp\")])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.resize(image, (224, 224))  \n",
    "        image = image[..., np.newaxis]  \n",
    "        image = image.astype(np.float32) / 255.0  \n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        try:\n",
    "            img_number = int(img_name.split(\".\")[0])  \n",
    "            person_id = ((img_number - 1) // 20) + 1  \n",
    "            label = person_id - 1  \n",
    "            if not (0 <= label <= 299):\n",
    "                raise ValueError(f\"Label {label + 1} out of range (1 to 300) for {img_name}\")\n",
    "        except (ValueError, IndexError):\n",
    "            raise ValueError(f\"Invalid filename format for label: {img_name}\")\n",
    "        return image, label, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0638cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified AlexNet for grayscale input\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        original_model = models.alexnet(pretrained=True)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2), \n",
    "            *original_model.features[1:]  \n",
    "        )\n",
    "        # Copy weights for conv1 (average RGB weights for grayscale)\n",
    "        with torch.no_grad():\n",
    "            self.features[0].weight.copy_(original_model.features[0].weight.mean(dim=1, keepdim=True))\n",
    "            self.features[0].bias.copy_(original_model.features[0].bias)\n",
    "        self.avgpool = original_model.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if return_features:\n",
    "            x = self.classifier[:6](x) \n",
    "            return x\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c25f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune AlexNet on Tonji\n",
    "def finetune_alexnet(model, train_loader, device, epochs=12):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)  \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y, _ in tqdm(train_loader, desc=f\"Fine-tuning Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        print(f\"Fine-tuning Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31f72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from a dataloader\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    img_names = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, names in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            x = x.to(device)\n",
    "            f = model(x, return_features=True).cpu().numpy()\n",
    "            features_list.append(f)\n",
    "            labels_list.extend(y.numpy())\n",
    "            img_names.extend(names)\n",
    "    features = np.concatenate(features_list, axis=0)\n",
    "    labels = np.array(labels_list)\n",
    "    return features, labels, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30721dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics (Top-1, Top-5, ROC AUC, EER, FAR, FRR)\n",
    "def compute_metrics(features, labels, clf):\n",
    "    decision_scores = clf.decision_function(features)  \n",
    "    topk_acc = {}\n",
    "    for k in [1, 5]:\n",
    "        top_k_indices = np.argsort(decision_scores, axis=1)[:, -k:]\n",
    "        correct = 0\n",
    "        for i, top_k in enumerate(top_k_indices):\n",
    "            if labels[i] in top_k:\n",
    "                correct += 1\n",
    "        topk_acc[k] = correct / len(labels)\n",
    "    y_true, y_score = [], []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            score_i = decision_scores[i, labels[i]]\n",
    "            score_j = decision_scores[j, labels[j]]\n",
    "            sim = score_i + score_j\n",
    "            y_score.append(sim)\n",
    "            y_true.append(1 if labels[i] == labels[j] else 0)\n",
    "    if len(set(y_true)) <= 1:\n",
    "        print(\"Warning: y_true contains only one class. Setting ROC AUC, EER, FAR, FRR to 0.\")\n",
    "        roc_auc = 0.1\n",
    "        eer = 0.1\n",
    "        far = 0.1\n",
    "        frr = 0.1\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        diff = np.abs(fnr - fpr)\n",
    "        if np.all(np.isnan(diff)):\n",
    "            print(\"Warning: All-NaN slice in EER calculation. Setting EER, FAR, FRR to 0.\")\n",
    "            eer = 0.1\n",
    "            far = 0.1\n",
    "            frr = 0.1\n",
    "        else:\n",
    "            eer_idx = np.nanargmin(diff)\n",
    "            eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "            far = fpr[eer_idx]\n",
    "            frr = fnr[eer_idx]\n",
    "    return topk_acc, roc_auc, eer, far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e685c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session1\"\n",
    "session2_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdb41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2105e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for Tonji\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40c41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tongji datasets\n",
    "dataset1 = PalmROIDataset(session1_root, transform=transform)\n",
    "dataset2 = PalmROIDataset(session2_root, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faebce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 images: 6000\n",
      "Session 2 images: 6000\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset sizes\n",
    "print(f\"Session 1 images: {len(dataset1)}\")\n",
    "print(f\"Session 2 images: {len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895a0067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 12000\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "full_dataset = ConcatDataset([dataset1, dataset2])\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c435dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789bd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51dedcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiteshk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiteshk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(num_classes=1000).to(device)  \n",
    "model.classifier[6] = nn.Linear(4096, 300).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e40a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1: 100%|██████████| 150/150 [00:55<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1, Loss: 5.7009, Accuracy: 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2: 100%|██████████| 150/150 [00:50<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2, Loss: 5.5978, Accuracy: 0.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 3: 100%|██████████| 150/150 [00:50<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 3, Loss: 5.4198, Accuracy: 1.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 4: 100%|██████████| 150/150 [00:59<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 4, Loss: 5.1325, Accuracy: 2.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 5: 100%|██████████| 150/150 [01:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 5, Loss: 4.8107, Accuracy: 4.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 6: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 6, Loss: 4.4459, Accuracy: 7.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 7: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 7, Loss: 4.0195, Accuracy: 11.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 8: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 8, Loss: 3.5054, Accuracy: 18.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 9: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 9, Loss: 2.8638, Accuracy: 28.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 10: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 10, Loss: 2.2477, Accuracy: 41.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 11: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 11, Loss: 1.6720, Accuracy: 54.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 12: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 12, Loss: 1.2453, Accuracy: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 13: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 13, Loss: 0.9464, Accuracy: 72.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 14: 100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 14, Loss: 0.7492, Accuracy: 78.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 15: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 15, Loss: 0.5805, Accuracy: 82.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 16: 100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 16, Loss: 0.4939, Accuracy: 85.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 17: 100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 17, Loss: 0.3900, Accuracy: 87.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 18: 100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 18, Loss: 0.3339, Accuracy: 89.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 19: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 19, Loss: 0.2899, Accuracy: 90.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 20: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 20, Loss: 0.2415, Accuracy: 92.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 21: 100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 21, Loss: 0.2371, Accuracy: 92.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 22: 100%|██████████| 150/150 [00:49<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 22, Loss: 0.1916, Accuracy: 94.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 23: 100%|██████████| 150/150 [00:49<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 23, Loss: 0.1955, Accuracy: 94.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 24: 100%|██████████| 150/150 [00:51<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 24, Loss: 0.1732, Accuracy: 94.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 25: 100%|██████████| 150/150 [00:53<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 25, Loss: 0.1504, Accuracy: 95.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 26: 100%|██████████| 150/150 [00:50<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 26, Loss: 0.1580, Accuracy: 95.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 27: 100%|██████████| 150/150 [00:51<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 27, Loss: 0.1301, Accuracy: 95.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 28: 100%|██████████| 150/150 [00:50<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 28, Loss: 0.1258, Accuracy: 96.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 29: 100%|██████████| 150/150 [00:50<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 29, Loss: 0.1220, Accuracy: 96.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 30: 100%|██████████| 150/150 [00:51<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 30, Loss: 0.0999, Accuracy: 96.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune AlexNet on Tonji\n",
    "finetune_alexnet(model, train_loader, device, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f1925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 150/150 [00:46<00:00,  3.24it/s]\n",
      "Extracting features: 100%|██████████| 38/38 [00:14<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "train_features, train_labels, _ = extract_features(model, train_loader, device)\n",
    "test_features, test_labels, _ = extract_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e77284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 300\n"
     ]
    }
   ],
   "source": [
    "# Verify number of classes\n",
    "num_classes = len(np.unique(np.concatenate((train_labels, test_labels))))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "if num_classes != 300:\n",
    "    raise ValueError(f\"Expected 300 classes, but found {num_classes}. Verify filename format (e.g., '00001.bmp' to '12000.bmp').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38fff9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "test_features = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253f14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF kernel and hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88df88e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 10, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr', probability=False)\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f\"Best SVM parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117baeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator\n",
    "svm_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d765e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute metrics\n",
    "topk_acc, roc_auc, eer, far, frr = compute_metrics(test_features, test_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f507bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Identification Accuracy: 95.54%\n",
      "Top-5 Identification Accuracy: 99.38%\n",
      "ROC AUC: 0.4958\n",
      "EER: 0.5039\n",
      "FAR: 0.5039\n",
      "FRR: 0.5039\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print(f\"Top-1 Identification Accuracy: {topk_acc[1]*100:.2f}%\")\n",
    "print(f\"Top-5 Identification Accuracy: {topk_acc[5]*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "print(f\"FAR: {far:.4f}\")\n",
    "print(f\"FRR: {frr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
