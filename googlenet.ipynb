{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1303db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0a05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader for Tonji\n",
    "class PalmROIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_list = sorted([f for f in os.listdir(root_dir) if f.endswith(\".bmp\")])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.resize(image, (224, 224))  # GoogLeNet expects 224x224 input\n",
    "        image = image[..., np.newaxis]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Extract person ID\n",
    "        try:\n",
    "            img_number = int(img_name.split(\".\")[0])\n",
    "            person_id = ((img_number - 1) // 20) + 1\n",
    "            label = person_id - 1\n",
    "            if not (0 <= label <= 299):\n",
    "                raise ValueError(f\"Label {label + 1} out of range (1 to 300) for {img_name}\")\n",
    "        except (ValueError, IndexError):\n",
    "            raise ValueError(f\"Invalid filename format for label: {img_name}\")\n",
    "        return image, label, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4505ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNetFeature(nn.Module):\n",
    "    def __init__(self, num_classes=300):\n",
    "        super(GoogLeNetFeature, self).__init__()\n",
    "        # Load pretrained GoogLeNet\n",
    "        self.googlenet = models.googlenet(weights='IMAGENET1K_V1')\n",
    "        # Average pretrained conv1 weights for grayscale adaptation\n",
    "        with torch.no_grad():\n",
    "            pretrained_weight = self.googlenet.conv1.conv.weight  \n",
    "            averaged_weight = pretrained_weight.mean(dim=1, keepdim=True)  \n",
    "            replicated_weight = averaged_weight.repeat(1, 3, 1, 1)  \n",
    "            self.googlenet.conv1.conv.weight.copy_(replicated_weight)\n",
    "        # Create feature extractor (all layers except final fc)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.googlenet.children())[:-1])\n",
    "        self.feature_extractor.add_module('dropout', self.googlenet.dropout)\n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Replicate 1-channel to 3 channels\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        # Extract features\n",
    "        x = self.feature_extractor(x)\n",
    "        # Flatten to [batch_size, 1024]\n",
    "        x = torch.flatten(x, 1)\n",
    "        if return_features:\n",
    "            return x  \n",
    "        # Apply classifier\n",
    "        x = self.classifier(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479c5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_googlenet(model, train_loader, device, epochs=40):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer with differential learning rates (no overlap)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters(), 'lr': 0.0001},  \n",
    "        {'params': model.classifier.parameters(), 'lr': 0.001},         \n",
    "    ])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    model.train()\n",
    "    prev_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y, _ in tqdm(train_loader, desc=f\"Fine-Tuning Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Fine-Tuning Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "        if any(lr != prev_lr[i] for i, lr in enumerate(current_lr)):\n",
    "            print(f\"Epoch {epoch+1}: Learning rates updated to {current_lr}\")\n",
    "        prev_lr = current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7be7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    img_names = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, names in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            x = x.to(device)\n",
    "            f = model(x, return_features=True).cpu().numpy()\n",
    "            features_list.append(f)\n",
    "            labels_list.extend(y.numpy())\n",
    "            img_names.extend(names)\n",
    "    features = np.concatenate(features_list, axis=0)\n",
    "    labels = np.array(labels_list)\n",
    "    return features, labels, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2680c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def compute_metrics(features, labels, clf):\n",
    "    decision_scores = clf.decision_function(features)\n",
    "    topk_acc = {}\n",
    "    for k in [1, 5]:\n",
    "        top_k_indices = np.argsort(decision_scores, axis=1)[:, -k:]\n",
    "        correct = 0\n",
    "        for i, top_k in enumerate(top_k_indices):\n",
    "            if labels[i] in top_k:\n",
    "                correct += 1\n",
    "        topk_acc[k] = correct / len(labels)\n",
    "    y_true, y_score = [], []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            score_i = decision_scores[i, labels[i]]\n",
    "            score_j = decision_scores[j, labels[j]]\n",
    "            sim = score_i + score_j\n",
    "            y_score.append(sim)\n",
    "            y_true.append(1 if labels[i] == labels[j] else 0)\n",
    "    if len(set(y_true)) <= 1:\n",
    "        print(\"Warning: y_true contains only one class. Setting ROC AUC, EER, FAR, FRR to 0.1.\")\n",
    "        roc_auc = 0.1\n",
    "        eer = 0.1\n",
    "        far = 0.1\n",
    "        frr = 0.1\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        diff = np.abs(fnr - fpr)\n",
    "        if np.all(np.isnan(diff)):\n",
    "            print(\"Warning: All-NaN slice in EER calculation. Setting EER, FAR, FRR to 0.1.\")\n",
    "            eer = 0.1\n",
    "            far = 0.1\n",
    "            frr = 0.1\n",
    "        else:\n",
    "            eer_idx = np.nanargmin(diff)\n",
    "            eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "            far = fpr[eer_idx]\n",
    "            frr = fnr[eer_idx]\n",
    "    return topk_acc, roc_auc, eer, far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d714a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code\n",
    "session1_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session1\"\n",
    "session2_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f82833",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bc2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for Tonji\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67069b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tongji datasets\n",
    "dataset1 = PalmROIDataset(session1_root, transform=transform)\n",
    "dataset2 = PalmROIDataset(session2_root, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0890a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 images: 6000\n",
      "Session 2 images: 6000\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset sizes\n",
    "print(f\"Session 1 images: {len(dataset1)}\")\n",
    "print(f\"Session 2 images: {len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3619f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 12000\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "full_dataset = ConcatDataset([dataset1, dataset2])\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44cf3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415278aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b27f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 1: 100%|██████████| 300/300 [01:42<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 1, Loss: 5.0340, Accuracy: 8.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2: 100%|██████████| 300/300 [01:46<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2, Loss: 1.5430, Accuracy: 72.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3: 100%|██████████| 300/300 [01:40<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3, Loss: 0.3165, Accuracy: 97.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4: 100%|██████████| 300/300 [01:36<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4, Loss: 0.1082, Accuracy: 99.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5: 100%|██████████| 300/300 [01:37<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5, Loss: 0.0515, Accuracy: 99.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6: 100%|██████████| 300/300 [01:37<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6, Loss: 0.0326, Accuracy: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7: 100%|██████████| 300/300 [01:36<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7, Loss: 0.0217, Accuracy: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8: 100%|██████████| 300/300 [01:37<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8, Loss: 0.0146, Accuracy: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9: 100%|██████████| 300/300 [01:37<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9, Loss: 0.0097, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10: 100%|██████████| 300/300 [01:38<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10, Loss: 0.0100, Accuracy: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 11: 100%|██████████| 300/300 [01:37<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 11, Loss: 0.0062, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 12: 100%|██████████| 300/300 [01:36<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 12, Loss: 0.0076, Accuracy: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 13: 100%|██████████| 300/300 [01:38<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 13, Loss: 0.0068, Accuracy: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 14: 100%|██████████| 300/300 [01:37<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 14, Loss: 0.0057, Accuracy: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 15: 100%|██████████| 300/300 [01:36<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 15, Loss: 0.0036, Accuracy: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GoogLeNetFeature(num_classes=300).to(device)\n",
    "fine_tune_googlenet(model, train_loader, device, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0c6696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 300/300 [01:02<00:00,  4.80it/s]\n",
      "Extracting features: 100%|██████████| 75/75 [00:18<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "train_features, train_labels, _ = extract_features(model, train_loader, device)\n",
    "test_features, test_labels, _ = extract_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d03abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 300\n"
     ]
    }
   ],
   "source": [
    "# Verify number of classes\n",
    "num_classes = len(np.unique(np.concatenate((train_labels, test_labels))))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "if num_classes != 300:\n",
    "    raise ValueError(f\"Expected 300 classes, but found {num_classes}. Verify filename format (e.g., '00001.bmp' to '12000.bmp').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "337c8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "test_features = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18dc2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF kernel and hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21076af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 1, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr', probability=False)\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f\"Best SVM parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e59acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator\n",
    "svm_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b340781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "topk_acc, roc_auc, eer, far, frr = compute_metrics(test_features, test_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae17cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Identification Accuracy: 99.88%\n",
      "Top-5 Identification Accuracy: 99.96%\n",
      "ROC AUC: 0.4929\n",
      "EER: 0.5058\n",
      "FAR: 0.5059\n",
      "FRR: 0.5058\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print(f\"Top-1 Identification Accuracy: {topk_acc[1]*100:.2f}%\")\n",
    "print(f\"Top-5 Identification Accuracy: {topk_acc[5]*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "print(f\"FAR: {far:.4f}\")\n",
    "print(f\"FRR: {frr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
